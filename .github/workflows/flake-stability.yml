name: Flake Stability (Reusable)

on:
  workflow_call:
    inputs:
      mode:
        required: true
        type: string

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  flake-detection:
    if: ${{ inputs.mode == 'detect' }}
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node + pnpm
        uses: ./.github/actions/setup-node-pnpm
        with:
          node-version: '20'
      - run: pnpm install --frozen-lockfile || pnpm install --no-frozen-lockfile
      - name: Install bc for calculations
        run: sudo apt-get update && sudo apt-get install -y bc
      - name: Capture environment details
        run: |
          mkdir -p reports/flake-detection
          NODE_VERSION="$(node -v)"
          PNPM_VERSION="$(pnpm -v)"
          VITEST_VERSION="$(pnpm exec vitest --version || printf '%s\n' 'unknown')"
          RUN_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"
          {
            printf "%s\n" "NODE_VERSION=$NODE_VERSION"
            printf "%s\n" "PNPM_VERSION=$PNPM_VERSION"
            printf "%s\n" "VITEST_VERSION=$VITEST_VERSION"
            printf "%s\n" "RUN_URL=$RUN_URL"
          } >> "$GITHUB_ENV"
          jq -n \
            --arg node "$NODE_VERSION" \
            --arg pnpm "$PNPM_VERSION" \
            --arg vitest "$VITEST_VERSION" \
            --arg url "$RUN_URL" \
            '{node: $node, pnpm: $pnpm, vitest: $vitest, runUrl: $url}' \
            > reports/flake-detection-env.json

      - name: Run tests multiple times
        id: flake-test
        run: |
          set -e
          fails=0
          total_runs=3

          printf "%s\n" "ğŸ” Starting flake detection with $total_runs runs"

          for ((i=1; i<=total_runs; i++)); do
            printf "%s\n" "ğŸ“Š Run #$i of $total_runs"
            report_file="reports/flake-detection/run-$i.json"
            if pnpm exec vitest run --workspace configs/vitest/vitest.workspace.ts --project integration --reporter=json --outputFile "$report_file"; then
              printf "%s\n" "âœ… Run #$i passed"
            else
              printf "%s\n" "âŒ Run #$i failed"
              fails=$((fails+1))
            fi

            # Brief pause between runs
            sleep 5
          done

          {
            printf "%s\n" "FAILS=$fails"
            printf "%s\n" "TOTAL_RUNS=$total_runs"
          } >> "$GITHUB_ENV"

          failure_rate=$(printf "%s\n" "scale=2; $fails / $total_runs * 100" | bc -l)
          printf "%s\n" "FAILURE_RATE=$failure_rate" >> "$GITHUB_ENV"

          printf "%s\n" "ğŸ“ˆ Flake detection summary:"
          printf "%s\n" "   Total runs: $total_runs"
          printf "%s\n" "   Failures: $fails"
          printf "%s\n" "   Failure rate: ${failure_rate}%"

      - name: Summarize failing tests
        if: always()
        run: |
          node <<'NODE'
          const fs = require('fs');
          const path = require('path');

          const dir = 'reports/flake-detection';
          const files = fs.existsSync(dir)
            ? fs.readdirSync(dir)
              .filter(name => name.startsWith('run-') && name.endsWith('.json'))
              .map(name => path.join(dir, name))
            : [];

          const failureCounts = new Map();
          const invalidFiles = [];
          const runErrors = [];
          const MAX_RUN_ERRORS = 10;
          const isFailedStatus = (value) => {
            if (!value) return false;
            const normalized = String(value).toLowerCase();
            return ['failed', 'fail', 'error'].includes(normalized);
          };
          const normalizeError = (err) => {
            if (!err) return 'unknown error';
            if (typeof err === 'string') return err;
            if (err.message) return err.message;
            if (err.stack) return err.stack;
            try {
              return JSON.stringify(err);
            } catch {
              return String(err);
            }
          };
          for (const file of files) {
            let data;
            try {
              data = JSON.parse(fs.readFileSync(file, 'utf8'));
            } catch (error) {
              invalidFiles.push({
                file: path.basename(file),
                error: error && error.message ? error.message : String(error),
              });
              continue;
            }

            const suites = Array.isArray(data.testResults)
              ? data.testResults
              : Array.isArray(data.data && data.data.testResults)
                ? data.data.testResults
                : Array.isArray(data.results)
                  ? data.results
                  : [];
            const suiteErrors = [];
            if (Array.isArray(data.unhandledErrors)) {
              suiteErrors.push(...data.unhandledErrors);
            }
            if (Array.isArray(data.errors)) {
              suiteErrors.push(...data.errors);
            }
            if (data.error) {
              suiteErrors.push(data.error);
            }
            for (const error of suiteErrors) {
              runErrors.push(normalizeError(error));
            }
            for (const suite of suites) {
              const assertions = Array.isArray(suite.assertionResults)
                ? suite.assertionResults
                : Array.isArray(suite.tests)
                  ? suite.tests
                  : [];
              if (assertions.length > 0) {
                for (const test of assertions) {
                  const status = test.status || test.state || test.result;
                  const failed = Boolean(test.failed)
                    || isFailedStatus(status)
                    || Boolean(test.failureMessage)
                    || (Array.isArray(test.errors) && test.errors.length > 0);
                  if (failed) {
                    const name = test.fullName || [ ...(test.ancestorTitles || []), test.title || test.name ]
                      .filter(Boolean)
                      .join(' > ');
                    failureCounts.set(name, (failureCounts.get(name) || 0) + 1);
                  }
                }
              } else {
                const suiteStatus = suite.status || suite.state || suite.result;
                const suiteFailed = isFailedStatus(suiteStatus)
                  || suite.numFailingTests > 0
                  || suite.success === false
                  || Boolean(suite.failureMessage);
                if (!suiteFailed) {
                  continue;
                }
                const suiteName = suite.name || suite.testFilePath || 'unknown suite';
                const key = `[suite] ${suiteName}`;
                failureCounts.set(key, (failureCounts.get(key) || 0) + 1);
              }
            }
          }

          const totalRuns = files.length || 0;
          const parsedRuns = Math.max(totalRuns - invalidFiles.length, 0);
          const runDenominator = parsedRuns || totalRuns || 0;
          const runErrorCounts = new Map();
          for (const error of runErrors) {
            runErrorCounts.set(error, (runErrorCounts.get(error) || 0) + 1);
          }
          const topRunErrors = [...runErrorCounts.entries()]
            .sort((a, b) => b[1] - a[1])
            .slice(0, MAX_RUN_ERRORS);
          if (failureCounts.size === 0 && topRunErrors.length > 0) {
            for (const [error, count] of topRunErrors) {
              const key = `[run error] ${error}`;
              failureCounts.set(key, (failureCounts.get(key) || 0) + count);
            }
          }
          const failures = [...failureCounts.entries()]
            .sort((a, b) => b[1] - a[1])
            .map(([name, count]) => ({ name, count }));

          let markdown = failures.length
            ? failures.map(item => `- ${item.name} (failed ${item.count}/${runDenominator})`).join('\n')
            : '- å¤±æ•—ãƒ†ã‚¹ãƒˆã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ';
          const runErrorDenominator = runDenominator || totalRuns || 1;
          if (topRunErrors.length > 0) {
            const runErrorLines = topRunErrors
              .map(([message, count]) => `- [run error] ${message} (${count}/${runErrorDenominator})`)
              .join('\n');
            markdown += `\n\n#### Run-level errors\n${runErrorLines}`;
          }
          if (invalidFiles.length > 0) {
            const invalidList = invalidFiles.map(item => item.file).join(', ');
            markdown += `\n\n- âš ï¸ è§£æã«å¤±æ•—ã—ãŸãƒ¬ãƒãƒ¼ãƒˆ: ${invalidList} (${invalidFiles.length}/${totalRuns})`;
          }

          fs.writeFileSync('reports/flake-detection-failures.md', markdown);
          fs.writeFileSync(
            'reports/flake-detection-failures.json',
            JSON.stringify({
              totalRuns,
              parsedRuns,
              invalidFiles,
              failures,
              runErrors: topRunErrors.map(([message, count]) => ({ message, count })),
            }, null, 2),
          );

          console.log('Failing tests summary:');
          console.log(markdown);
          NODE

      - name: Create flake detection report
        run: |
          mkdir -p reports
          status="stable"
          if [ "$(printf "%s\n" "$FAILURE_RATE > 30.0" | bc -l)" -eq 1 ]; then
            status="flaky"
          fi

          cat > reports/flake-detection-report.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "totalRuns": $TOTAL_RUNS,
            "failures": $FAILS,
            "failureRate": $FAILURE_RATE,
            "threshold": 30.0,
            "status": "$status",
            "testSuite": "integration",
            "environment": "ci"
          }
          EOF

      - name: Check flake threshold
        id: flake-check
        run: |
          threshold=30.0

          if [ "$(printf "%s\n" "$FAILURE_RATE > $threshold" | bc -l)" -eq 1 ]; then
            printf "%s\n" "ğŸš¨ Flake detected! Failure rate: ${FAILURE_RATE}% exceeds threshold: ${threshold}%"
            printf "%s\n" "flaky=true" >> "$GITHUB_OUTPUT"
          else
            printf "%s\n" "âœ… Tests stable. Failure rate: ${FAILURE_RATE}% below threshold: ${threshold}%"
            printf "%s\n" "flaky=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Upload flake detection report
        uses: actions/upload-artifact@v4
        with:
          name: flake-detection-report
          path: |
            reports/flake-detection-report.json
            reports/flake-detection-failures.json
            reports/flake-detection-failures.md
            reports/flake-detection-env.json
            reports/flake-detection/run-*.json
          retention-days: 30

      - name: Create GitHub issue for flaky tests
        if: steps.flake-check.outputs.flaky == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const failureRate = '${{ env.FAILURE_RATE }}';
            const totalRuns = '${{ env.TOTAL_RUNS }}';
            const failures = '${{ env.FAILS }}';
            const fs = require('fs');
            let failureSummary = '';
            try {
              failureSummary = fs.readFileSync('reports/flake-detection-failures.md', 'utf8').trim();
            } catch (error) {
              failureSummary = '';
            }
            const failureSection = failureSummary
              ? `### â— Failing tests (latest runs)\n${failureSummary}\n`
              : '### â— Failing tests (latest runs)\n- Summary not available. See flake-detection-report artifacts.\n';
            const envSection = `### ğŸ§ª Environment\n- Node: ${process.env.NODE_VERSION || 'unknown'}\n- pnpm: ${process.env.PNPM_VERSION || 'unknown'}\n- Vitest: ${process.env.VITEST_VERSION || 'unknown'}\n- Run: ${process.env.RUN_URL || 'unknown'}\n`;
            const reproSection = `### â–¶ï¸ Reproduction command\n\`pnpm exec vitest run --workspace configs/vitest/vitest.workspace.ts --project integration --reporter=json\`\n`;

            const title = `ğŸ” Flaky Test Detected - Integration Tests (${failureRate}% failure rate)`;

            const body = `## ğŸš¨ Flaky Test Detection Report

            **Test Suite:** Integration Tests  
            **Detection Time:** ${new Date().toISOString()}  
            **Failure Rate:** ${failureRate}% (${failures}/${totalRuns} runs failed)  
            **Threshold:** 30.0%

            ### ğŸ“Š Test Run Results
            - **Total Runs:** ${totalRuns}
            - **Failed Runs:** ${failures}
            - **Success Runs:** ${totalRuns - failures}
            - **Failure Rate:** ${failureRate}%

            ${failureSection}

            ${envSection}

            ${reproSection}

            ### ğŸ¯ Recommended Actions
            1. **Immediate:** Isolate flaky tests to prevent CI blocking
            2. **Short-term:** Investigate test timing and resource dependencies
            3. **Long-term:** Implement test retry mechanisms or improve test stability

            ### ğŸ”§ Investigation Steps
            - [ ] Check test logs for timing issues
            - [ ] Review resource cleanup in test teardown
            - [ ] Analyze system load during test execution
            - [ ] Consider test isolation improvements

            ### ğŸ“‹ Labels Applied
            - \`flaky-test\`: Test exhibits non-deterministic behavior
            - \`ci-stability\`: Affects continuous integration stability
            - \`priority-high\`: High priority due to CI impact

            **Auto-generated by Flake Detection Workflow**  
            *Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}*`;

            // Check if similar issue already exists
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'flaky-test',
              state: 'open'
            });

            const duplicateIssue = existingIssues.data.find(issue =>
              issue.title.includes('Integration Tests') &&
              issue.title.includes('Flaky Test Detected')
            );

            if (duplicateIssue) {
              console.log(`Updating existing issue #${duplicateIssue.number}`);

              const updateBody = `${duplicateIssue.body}

              ---

              ## ğŸ”„ Recent Detection - ${new Date().toISOString()}
              **Failure Rate:** ${failureRate}% (${failures}/${totalRuns} runs failed)`;

              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: duplicateIssue.number,
                body: updateBody
              });

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: duplicateIssue.number,
                body: `ğŸš¨ **Flake still detected** - ${failureRate}% failure rate in latest run${failureSummary ? `\n\nFailing tests:\n${failureSummary}` : ''}`
              });

            } else {
              console.log('Creating new flaky test issue');

              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['flaky-test', 'ci-stability', 'priority-high', 'automated']
              });

              console.log(`Created issue #${issue.data.number}`);
            }

      - name: Comment on existing flake issue when stable
        if: steps.flake-check.outputs.flaky == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const failureRate = '${{ env.FAILURE_RATE }}';
            const totalRuns = '${{ env.TOTAL_RUNS }}';
            const failures = '${{ env.FAILS }}';
            const timestamp = new Date().toISOString();
            const envSection = `### ğŸ§ª Environment\n- Node: ${process.env.NODE_VERSION || 'unknown'}\n- pnpm: ${process.env.PNPM_VERSION || 'unknown'}\n- Vitest: ${process.env.VITEST_VERSION || 'unknown'}\n- Run: ${process.env.RUN_URL || 'unknown'}\n`;

            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'flaky-test',
              state: 'open'
            });

            const targetIssue = existingIssues.data.find(issue =>
              issue.title.includes('Integration Tests') &&
              issue.title.includes('Flaky Test Detected')
            );

            if (!targetIssue) {
              console.log('No open flaky-test issue found for Integration Tests.');
              return;
            }

            const body = `âœ… **Stable run** - ${failureRate}% failure rate (${failures}/${totalRuns})\n\n**Timestamp:** ${timestamp}\n\n${envSection}`;
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: targetIssue.number,
              body
            });

      - name: Report final status
        run: |
          if [ "${{ steps.flake-check.outputs.flaky }}" = "true" ]; then
            printf "%s\n" "âŒ Flake detection workflow completed - FLAKY TESTS DETECTED"
            printf "%s\n" "Failure rate: ${FAILURE_RATE}% exceeds threshold"
            exit 1
          else
            printf "%s\n" "âœ… Flake detection workflow completed - Tests are STABLE"
            printf "%s\n" "Failure rate: ${FAILURE_RATE}% below threshold"
          fi

  maintenance:
    if: ${{ inputs.mode == 'maintenance' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4
      - name: Setup Node + pnpm
        uses: ./.github/actions/setup-node-pnpm
        with:
          node-version: '20'
      - run: pnpm install --frozen-lockfile || pnpm install --no-frozen-lockfile

      - name: Install bc for calculations
        run: sudo apt-get update && sudo apt-get install -y bc

      - name: Run flake maintenance
        run: |
          printf "%s\n" "ğŸ”§ Running daily flake isolation maintenance..."
          pnpm run flake:maintenance

      - name: Generate maintenance report
        run: |
          printf "%s\n" "ğŸ“Š Generating maintenance report..."
          pnpm run flake:report

          # Display current status
          printf "%s\n" "ğŸ“‹ Current flake isolation status:"
          pnpm run flake:list

      - name: Upload maintenance reports
        uses: actions/upload-artifact@v4
        with:
          name: flake-maintenance-report-${{ github.run_id }}
          path: |
            reports/flake-reports/
            config/flaky-tests.json
            config/test-patterns.json
          retention-days: 30

      - name: Check for recovery candidates
        id: recovery-check
        run: |
          # Check if any tests have been recovered
          if [ -f "reports/flake-reports/latest-flake-isolation-report.json" ]; then
            recovered_count=$(jq -r '.isolatedTests | map(select(.status == "recovered")) | length' reports/flake-reports/latest-flake-isolation-report.json)
            printf "%s\n" "recovered_count=$recovered_count" >> "$GITHUB_OUTPUT"
            printf "%s\n" "Found $recovered_count recovered tests"
          else
            printf "%s\n" "recovered_count=0" >> "$GITHUB_OUTPUT"
          fi

      - name: Create recovery notification
        if: steps.recovery-check.outputs.recovered_count > 0
        uses: actions/github-script@v7
        with:
          script: |
            const recoveredCount = '${{ steps.recovery-check.outputs.recovered_count }}';

            const title = `ğŸ‰ Flaky Test Recovery - ${recoveredCount} Tests Recovered`;

            const body = `## ğŸ‰ Flaky Test Recovery Notification

            **Recovery Date:** ${new Date().toISOString()}  
            **Tests Recovered:** ${recoveredCount}

            ### ğŸ“Š Recovery Summary
            The daily flake maintenance process has identified ${recoveredCount} previously flaky test(s) that have shown stable behavior and are candidates for re-integration into the main test suite.

            ### ğŸ¯ Next Steps
            1. **Review** the recovered tests in the flake isolation report
            2. **Consider** re-enabling these tests in the main CI pipeline
            3. **Monitor** their behavior closely after re-integration
            4. **Update** test patterns configuration if needed

            ### ğŸ“‹ Actions Required
            - [ ] Review flake isolation report
            - [ ] Test recovered patterns in isolation
            - [ ] Update CI test configuration
            - [ ] Monitor for continued stability

            **Auto-generated by Daily Flake Maintenance Workflow**  
            *Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}*`;

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['flaky-test', 'recovered', 'maintenance', 'automated']
            });

            console.log(`Created recovery notification issue`);

      - name: Update test patterns
        run: |
          printf "%s\n" "ğŸ”„ Checking if test patterns need updating..."

          if [ -f "config/test-patterns.json" ]; then
            printf "%s\n" "âœ… Test patterns configuration updated"
            git diff --name-only config/test-patterns.json || true
          else
            printf "%s\n" "âš ï¸ No test patterns configuration found"
          fi

      - name: Summary report
        run: |
          printf "%s\n" "ğŸ“‹ Daily Flake Maintenance Complete"
          printf "%s\n" "=================================="

          if [ -f "reports/flake-reports/latest-flake-isolation-report.json" ]; then
            printf "%s\n" "ğŸ“Š Current Status:"
            jq -r '.summary | to_entries | map(\"  \\(.key): \\(.value)\") | .[]' reports/flake-reports/latest-flake-isolation-report.json

            printf "%s\n" ""
            printf "%s\n" "ğŸ’¡ Recommendations:"
            jq -r '.recommendations[] | \"  [\\(.priority | ascii_upcase)] \\(.message)\"' reports/flake-reports/latest-flake-isolation-report.json
          else
            printf "%s\n" "âš ï¸ No flake isolation report found"
          fi

          printf "%s\n" "=================================="
