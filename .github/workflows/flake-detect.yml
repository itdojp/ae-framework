name: Flake Detect
on:
  workflow_dispatch:
  schedule:
    - cron: '0 21 * * *' # JST 06:00
permissions: 
  contents: read
  issues: write
  pull-requests: write
jobs:
  run3:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@v4
      - name: Prepare pnpm
        uses: ./.github/actions/setup-pnpm
      - uses: actions/setup-node@v4
        with: 
          node-version: '20'
          cache: 'pnpm'
      - run: corepack enable
      - run: pnpm install --frozen-lockfile || pnpm install --no-frozen-lockfile
      - name: Capture environment details
        run: |
          mkdir -p reports/flake-detection
          NODE_VERSION="$(node -v)"
          PNPM_VERSION="$(pnpm -v)"
          VITEST_VERSION="$(pnpm exec vitest --version || printf '%s\n' 'unknown')"
          RUN_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"
          {
            printf "%s\n" "NODE_VERSION=$NODE_VERSION"
            printf "%s\n" "PNPM_VERSION=$PNPM_VERSION"
            printf "%s\n" "VITEST_VERSION=$VITEST_VERSION"
            printf "%s\n" "RUN_URL=$RUN_URL"
          } >> "$GITHUB_ENV"
          jq -n \
            --arg node "$NODE_VERSION" \
            --arg pnpm "$PNPM_VERSION" \
            --arg vitest "$VITEST_VERSION" \
            --arg url "$RUN_URL" \
            '{node: $node, pnpm: $pnpm, vitest: $vitest, runUrl: $url}' \
            > reports/flake-detection-env.json
      
      - name: Run tests multiple times
        id: flake-test
        run: |
          set -e
          fails=0
          total_runs=3
          
          printf "%s\n" "üîç Starting flake detection with $total_runs runs"
          
          for ((i=1; i<=total_runs; i++)); do
            printf "%s\n" "üìä Run #$i of $total_runs"
            report_file="reports/flake-detection/run-$i.json"
            if pnpm exec vitest run --workspace configs/vitest/vitest.workspace.ts --project integration --reporter=json --outputFile "$report_file"; then
              printf "%s\n" "‚úÖ Run #$i passed"
            else
              printf "%s\n" "‚ùå Run #$i failed"
              fails=$((fails+1))
            fi
            
            # Brief pause between runs
            sleep 5
          done
          
          {
            printf "%s\n" "FAILS=$fails"
            printf "%s\n" "TOTAL_RUNS=$total_runs"
          } >> "$GITHUB_ENV"
          
          failure_rate=$(printf "%s\n" "scale=2; $fails / $total_runs * 100" | bc -l)
          printf "%s\n" "FAILURE_RATE=$failure_rate" >> "$GITHUB_ENV"
          
          printf "%s\n" "üìà Flake detection summary:"
          printf "%s\n" "   Total runs: $total_runs"
          printf "%s\n" "   Failures: $fails"
          printf "%s\n" "   Failure rate: ${failure_rate}%"

      - name: Summarize failing tests
        if: always()
        run: |
          node <<'NODE'
          const fs = require('fs');
          const path = require('path');

          const dir = 'reports/flake-detection';
          const files = fs.existsSync(dir)
            ? fs.readdirSync(dir)
              .filter(name => name.startsWith('run-') && name.endsWith('.json'))
              .map(name => path.join(dir, name))
            : [];

          const failureCounts = new Map();
          const invalidFiles = [];
          const runErrors = [];
          const MAX_RUN_ERRORS = 10;
          const isFailedStatus = (value) => {
            if (!value) return false;
            const normalized = String(value).toLowerCase();
            return ['failed', 'fail', 'error'].includes(normalized);
          };
          const normalizeError = (err) => {
            if (!err) return 'unknown error';
            if (typeof err === 'string') return err;
            if (err.message) return err.message;
            if (err.stack) return err.stack;
            try {
              return JSON.stringify(err);
            } catch {
              return String(err);
            }
          };
          for (const file of files) {
            let data;
            try {
              data = JSON.parse(fs.readFileSync(file, 'utf8'));
            } catch (error) {
              invalidFiles.push({
                file: path.basename(file),
                error: error && error.message ? error.message : String(error),
              });
              continue;
            }

            const suites = Array.isArray(data.testResults)
              ? data.testResults
              : Array.isArray(data.data && data.data.testResults)
                ? data.data.testResults
                : Array.isArray(data.results)
                  ? data.results
                  : [];
            const suiteErrors = [];
            if (Array.isArray(data.unhandledErrors)) {
              suiteErrors.push(...data.unhandledErrors);
            }
            if (Array.isArray(data.errors)) {
              suiteErrors.push(...data.errors);
            }
            if (data.error) {
              suiteErrors.push(data.error);
            }
            for (const error of suiteErrors) {
              runErrors.push(normalizeError(error));
            }
            for (const suite of suites) {
              const assertions = Array.isArray(suite.assertionResults)
                ? suite.assertionResults
                : Array.isArray(suite.tests)
                  ? suite.tests
                  : [];
              if (assertions.length > 0) {
                for (const test of assertions) {
                  const status = test.status || test.state || test.result;
                  const failed = Boolean(test.failed)
                    || isFailedStatus(status)
                    || Boolean(test.failureMessage)
                    || (Array.isArray(test.errors) && test.errors.length > 0);
                  if (failed) {
                    const name = test.fullName || [ ...(test.ancestorTitles || []), test.title || test.name ]
                      .filter(Boolean)
                      .join(' > ');
                    failureCounts.set(name, (failureCounts.get(name) || 0) + 1);
                  }
                }
              } else {
                const suiteStatus = suite.status || suite.state || suite.result;
                const suiteFailed = isFailedStatus(suiteStatus)
                  || suite.numFailingTests > 0
                  || suite.success === false
                  || Boolean(suite.failureMessage);
                if (!suiteFailed) {
                  continue;
                }
                const suiteName = suite.name || suite.testFilePath || 'unknown suite';
                const key = `[suite] ${suiteName}`;
                failureCounts.set(key, (failureCounts.get(key) || 0) + 1);
              }
            }
          }

          const totalRuns = files.length || 0;
          const parsedRuns = Math.max(totalRuns - invalidFiles.length, 0);
          const runDenominator = parsedRuns || totalRuns || 0;
          const runErrorCounts = new Map();
          for (const error of runErrors) {
            runErrorCounts.set(error, (runErrorCounts.get(error) || 0) + 1);
          }
          const topRunErrors = [...runErrorCounts.entries()]
            .sort((a, b) => b[1] - a[1])
            .slice(0, MAX_RUN_ERRORS);
          if (failureCounts.size === 0 && topRunErrors.length > 0) {
            for (const [error, count] of topRunErrors) {
              const key = `[run error] ${error}`;
              failureCounts.set(key, (failureCounts.get(key) || 0) + count);
            }
          }
          const failures = [...failureCounts.entries()]
            .sort((a, b) => b[1] - a[1])
            .map(([name, count]) => ({ name, count }));

          let markdown = failures.length
            ? failures.map(item => `- ${item.name} (failed ${item.count}/${runDenominator})`).join('\n')
            : '- Â§±Êïó„ÉÜ„Çπ„Éà„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü';
          const runErrorDenominator = runDenominator || totalRuns || 1;
          if (topRunErrors.length > 0) {
            const runErrorLines = topRunErrors
              .map(([message, count]) => `- [run error] ${message} (${count}/${runErrorDenominator})`)
              .join('\n');
            markdown += `\n\n#### Run-level errors\n${runErrorLines}`;
          }
          if (invalidFiles.length > 0) {
            const invalidList = invalidFiles.map(item => item.file).join(', ');
            markdown += `\n\n- ‚ö†Ô∏è Ëß£Êûê„Å´Â§±Êïó„Åó„Åü„É¨„Éù„Éº„Éà: ${invalidList} (${invalidFiles.length}/${totalRuns})`;
          }

          fs.writeFileSync('reports/flake-detection-failures.md', markdown);
          fs.writeFileSync(
            'reports/flake-detection-failures.json',
            JSON.stringify({
              totalRuns,
              parsedRuns,
              invalidFiles,
              failures,
              runErrors: topRunErrors.map(([message, count]) => ({ message, count })),
            }, null, 2),
          );

          console.log('Failing tests summary:');
          console.log(markdown);
          NODE

      - name: Create flake detection report
        run: |
          mkdir -p reports
          status="stable"
          if [ "$(printf "%s\n" "$FAILURE_RATE > 30.0" | bc -l)" -eq 1 ]; then
            status="flaky"
          fi

          cat > reports/flake-detection-report.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "totalRuns": $TOTAL_RUNS,
            "failures": $FAILS,
            "failureRate": $FAILURE_RATE,
            "threshold": 30.0,
            "status": "$status",
            "testSuite": "integration",
            "environment": "ci"
          }
          EOF
      
      - name: Check flake threshold
        id: flake-check
        run: |
          threshold=30.0
          
          if [ "$(printf "%s\n" "$FAILURE_RATE > $threshold" | bc -l)" -eq 1 ]; then
            printf "%s\n" "üö® Flake detected! Failure rate: ${FAILURE_RATE}% exceeds threshold: ${threshold}%"
            printf "%s\n" "flaky=true" >> "$GITHUB_OUTPUT"
          else
            printf "%s\n" "‚úÖ Tests stable. Failure rate: ${FAILURE_RATE}% below threshold: ${threshold}%"
            printf "%s\n" "flaky=false" >> "$GITHUB_OUTPUT"
          fi
      
      - name: Upload flake detection report
        uses: actions/upload-artifact@v4
        with:
          name: flake-detection-report
          path: |
            reports/flake-detection-report.json
            reports/flake-detection-failures.json
            reports/flake-detection-failures.md
            reports/flake-detection-env.json
            reports/flake-detection/run-*.json
          retention-days: 30
      
      - name: Create GitHub issue for flaky tests
        if: steps.flake-check.outputs.flaky == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const failureRate = '${{ env.FAILURE_RATE }}';
            const totalRuns = '${{ env.TOTAL_RUNS }}';
            const failures = '${{ env.FAILS }}';
            const fs = require('fs');
            let failureSummary = '';
            try {
              failureSummary = fs.readFileSync('reports/flake-detection-failures.md', 'utf8').trim();
            } catch (error) {
              failureSummary = '';
            }
            const failureSection = failureSummary
              ? `### ‚ùó Failing tests (latest runs)\n${failureSummary}\n`
              : '### ‚ùó Failing tests (latest runs)\n- Summary not available. See flake-detection-report artifacts.\n';
            const envSection = `### üß™ Environment\n- Node: ${process.env.NODE_VERSION || 'unknown'}\n- pnpm: ${process.env.PNPM_VERSION || 'unknown'}\n- Vitest: ${process.env.VITEST_VERSION || 'unknown'}\n- Run: ${process.env.RUN_URL || 'unknown'}\n`;
            const reproSection = `### ‚ñ∂Ô∏è Reproduction command\n\`pnpm exec vitest run --workspace configs/vitest/vitest.workspace.ts --project integration --reporter=json\`\n`;
            
            const title = `üîç Flaky Test Detected - Integration Tests (${failureRate}% failure rate)`;
            
            const body = `## üö® Flaky Test Detection Report
            
            **Test Suite:** Integration Tests  
            **Detection Time:** ${new Date().toISOString()}  
            **Failure Rate:** ${failureRate}% (${failures}/${totalRuns} runs failed)  
            **Threshold:** 30.0%
            
            ### üìä Test Run Results
            - **Total Runs:** ${totalRuns}
            - **Failed Runs:** ${failures}
            - **Success Runs:** ${totalRuns - failures}
            - **Failure Rate:** ${failureRate}%

            ${failureSection}
            
            ${envSection}
            
            ${reproSection}
            
            ### üéØ Recommended Actions
            1. **Immediate:** Isolate flaky tests to prevent CI blocking
            2. **Short-term:** Investigate test timing and resource dependencies
            3. **Long-term:** Implement test retry mechanisms or improve test stability
            
            ### üîß Investigation Steps
            - [ ] Check test logs for timing issues
            - [ ] Review resource cleanup in test teardown
            - [ ] Analyze system load during test execution
            - [ ] Consider test isolation improvements
            
            ### üìã Labels Applied
            - \`flaky-test\`: Test exhibits non-deterministic behavior
            - \`ci-stability\`: Affects continuous integration stability
            - \`priority-high\`: High priority due to CI impact
            
            **Auto-generated by Flake Detection Workflow**  
            *Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}*`;
            
            // Check if similar issue already exists
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'flaky-test',
              state: 'open'
            });
            
            const duplicateIssue = existingIssues.data.find(issue => 
              issue.title.includes('Integration Tests') && 
              issue.title.includes('Flaky Test Detected')
            );
            
            if (duplicateIssue) {
              console.log(`Updating existing issue #${duplicateIssue.number}`);
              
              const updateBody = `${duplicateIssue.body}
              
              ---
              
              ## üîÑ Recent Detection - ${new Date().toISOString()}
              **Failure Rate:** ${failureRate}% (${failures}/${totalRuns} runs failed)`;
              
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: duplicateIssue.number,
                body: updateBody
              });
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: duplicateIssue.number,
                body: `üö® **Flake still detected** - ${failureRate}% failure rate in latest run${failureSummary ? `\n\nFailing tests:\n${failureSummary}` : ''}`
              });
              
            } else {
              console.log('Creating new flaky test issue');
              
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['flaky-test', 'ci-stability', 'priority-high', 'automated']
              });
              
              console.log(`Created issue #${issue.data.number}`);
            }

      - name: Comment on existing flake issue when stable
        if: steps.flake-check.outputs.flaky == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const failureRate = '${{ env.FAILURE_RATE }}';
            const totalRuns = '${{ env.TOTAL_RUNS }}';
            const failures = '${{ env.FAILS }}';
            const envSection = `### üß™ Environment\n- Node: ${process.env.NODE_VERSION || 'unknown'}\n- pnpm: ${process.env.PNPM_VERSION || 'unknown'}\n- Vitest: ${process.env.VITEST_VERSION || 'unknown'}\n- Run: ${process.env.RUN_URL || 'unknown'}\n`;

            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'flaky-test',
              state: 'open'
            });

            const targetIssue = existingIssues.data.find(issue =>
              issue.title.includes('Integration Tests') &&
              issue.title.includes('Flaky Test Detected')
            );

            if (!targetIssue) {
              console.log('No open flaky-test issue found for Integration Tests.');
              return;
            }

            const body = `‚úÖ **Stable run** - ${failureRate}% failure rate (${failures}/${totalRuns})\\n\\n${envSection}`;
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: targetIssue.number,
              body
            });
      
      - name: Report final status
        run: |
          if [ "${{ steps.flake-check.outputs.flaky }}" = "true" ]; then
            printf "%s\n" "‚ùå Flake detection workflow completed - FLAKY TESTS DETECTED"
            printf "%s\n" "Failure rate: ${FAILURE_RATE}% exceeds threshold"
            exit 1
          else
            printf "%s\n" "‚úÖ Flake detection workflow completed - Tests are STABLE"
            printf "%s\n" "Failure rate: ${FAILURE_RATE}% below threshold"
          fi
